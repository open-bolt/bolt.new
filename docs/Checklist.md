# Implementation Checklist for Bolt.new Vercel AI SDK Upgrade

Below is a detailed, step-by-step checklist aligned with the PRD milestones. Each section corresponds to a milestone (Initial Integration, UI/UX Updates, Beta Release, Final Release) and includes actionable development steps. Relevant Vercel AI SDK documentation and research references are cited for guidance.

## Milestone 1: Initial Integration

- [ ] **Install & Configure Vercel AI SDK (v4.2+):** Add the Vercel AI SDK Core and UI packages to the project (e.g. via `npm`/`pnpm`). Ensure the version is 4.2 or above, which provides unified provider management and tool APIs ([AI SDK Core: Provider Management](https://sdk.vercel.ai/docs/ai-sdk-core/provider-management#:~:text=When%20you%20work%20with%20multiple,models%20through%20simple%20string%20ids)). This will serve as the core layer for all AI interactions (replacing any direct calls to providers).  
  *Dependency:* Understanding the SDK’s basics and how it standardizes model interfaces to avoid vendor lock-in ([AI SDK Core: Provider Management](https://sdk.vercel.ai/docs/ai-sdk-core/provider-management#:~:text=When%20you%20work%20with%20multiple,models%20through%20simple%20string%20ids)).

- [ ] **Set Up Provider Registry with Claude Default:** Utilize the SDK’s provider registry to register available AI providers and models ([AI SDK Core: Provider Management](https://sdk.vercel.ai/docs/ai-sdk-core/provider-management#:~:text=When%20you%20work%20with%20multiple,models%20through%20simple%20string%20ids)). Configure **Anthropic’s Claude** as the default model in this registry (using the Anthropics SDK module, e.g. `anthropic('<model-name>')`) ([Guides: Get started with Claude 3.7 Sonnet](https://sdk.vercel.ai/docs/guides/sonnet-3-7#:~:text=model%3A%20anthropic%28%27claude)). For now, point to the latest Claude model (e.g. Claude 2 or Claude 3.* Sonnet) and load the Anthropics API key from environment variables.  
  *Subtasks:* 
  - [ ] Import the Anthropics provider (`import { anthropic } from '@ai-sdk/anthropic';`) and initialize it with the chosen model ID ([Guides: Get started with Claude 3.7 Sonnet](https://sdk.vercel.ai/docs/guides/sonnet-3-7#:~:text=model%3A%20anthropic%28%27claude)).  
  - [ ] Use `experimental_createProviderRegistry` (or similar registry API) to create a registry mapping a provider key (like `"anthropic"`) to the Anthropics model instance ([AI SDK Core: Provider Management](https://sdk.vercel.ai/docs/ai-sdk-core/provider-management#:~:text=When%20you%20work%20with%20multiple,models%20through%20simple%20string%20ids)). This allows switching providers by a string ID in future.  
  - [ ] (Optional) Stub other providers for future expansion: e.g., import OpenAI (`@ai-sdk/openai`) and include an entry in the registry (with OpenAI’s `GPT-4` or others), though not default. This sets the stage for easy expansion to OpenAI or others later ([AI SDK Core: Provider Management](https://sdk.vercel.ai/docs/ai-sdk-core/provider-management#:~:text=When%20you%20work%20with%20multiple,models%20through%20simple%20string%20ids)).  
  *Dependency:* An `.env` file with `ANTHROPIC_API_KEY` set (and `OPENAI_API_KEY` if stubbing OpenAI). Ensure keys are **not** hard-coded – use environment variables as shown in the `env.example` (e.g., `ANTHROPIC_API_KEY=...`) ([bolt.new/env.example at main · open-bolt/bolt.new · GitHub](https://github.com/open-bolt/bolt.new/blob/main/env.example#:~:text=API%20Keys)).

- [ ] **Refactor to Use Unified Model API:** Update all AI invocation points in the codebase to use the Vercel SDK’s unified API. For example, replace direct fetches to Anthropic/OpenAI with the SDK’s `generateText` or `streamText` functions, supplying the model via the provider registry (e.g., `model: '<anthropic-model-id>'`). This change centralizes AI calls through the SDK’s interface.  
  *Subtasks:* 
  - [ ] Remove any legacy or custom integration code for AI (such as old SDK calls) that is now supplanted by Vercel’s SDK ([AI SDK Core: Provider Management](https://sdk.vercel.ai/docs/ai-sdk-core/provider-management#:~:text=When%20you%20work%20with%20multiple,models%20through%20simple%20string%20ids)).  
  - [ ] Pass messages or prompts to the SDK’s `generateText`/`streamText` along with the `model` (using the default Claude provider) and any needed parameters (temperature, max tokens, etc.). This ensures Claude is the active model for all AI responses by default.  
  - [ ] Verify basic prompt/response flow still works using Claude via the new SDK (sanity test the integration with a simple prompt).

- [ ] **Implement Tool Calling (Basic):** Introduce the Vercel AI SDK’s **tool calling** features so the AI agent can execute helper functions. Start with one or two example tools to validate the flow.  
  - [ ] **Define Tools with Schema:** Use the SDK’s `tool` helper to define available tools, providing each tool’s `description`, `parameters` schema, and `execute` function. Leverage **Zod** schemas or JSON schema for the tool parameters to enforce correct argument structures ([AI SDK Core: Tool Calling](https://sdk.vercel.ai/docs/ai-sdk-core/tools-and-tool-calling#:~:text=,called%20with%20the%20arguments%20from)). For example, define a simple tool (e.g., `getWeather`) with a Zod schema for inputs (`location: string`) and an `execute` function that returns mock data.  
  - [ ] **Register Tools in AI Calls:** Include the tools in the `generateText`/`streamText` call via the `tools` option. Confirm that the AI can call the tool by prompting a query that would trigger it.  
  - [ ] **Enable Multi-Step Calls:** Allow the AI to perform reasoning in multiple steps by setting `maxSteps` > 1 in generation calls ([AI SDK Core: Tool Calling](https://sdk.vercel.ai/docs/ai-sdk-core/tools-and-tool-calling#:~:text=With%20the%20,of%20tool%20steps%20is%20reached)). This lets the model invoke a tool, get the result, and continue the conversation in a single prompt cycle. For instance, set `maxSteps: 5` to permit up to 5 tool usages + responses in one query ([AI SDK Core: Tool Calling](https://sdk.vercel.ai/docs/ai-sdk-core/tools-and-tool-calling#:~:text=With%20the%20,of%20tool%20steps%20is%20reached)). (This simulates an agentic loop where the AI can think, use a tool, then continue answering.)  
  - [ ] **Handle Tool Errors:** Implement robust error handling for tool calls using the SDK’s error classes. Catch and handle `NoSuchToolError` and `InvalidToolArgumentsError` so that if the model tries to call an undefined tool or passes wrong params, the system responds gracefully ([AI SDK Core: Tool Calling](https://sdk.vercel.ai/docs/ai-sdk-core/tools-and-tool-calling#:~:text=,not%20match%20the%20tool%27s%20parameters)). For example, log the error and return a friendly message like “(⚠️ The AI tried to use an unavailable tool)” rather than breaking the chat. Utilize the SDK’s error-handling guide as needed ([AI SDK Core: Tool Calling](https://sdk.vercel.ai/docs/ai-sdk-core/tools-and-tool-calling#:~:text=,not%20match%20the%20tool%27s%20parameters)).  
  *Dependency:* The tools should be relatively side-effect free and safe, since they will be invoked by the AI. Ensure each tool’s `execute` is sandboxed within the WebContainers environment as appropriate.

- [ ] **Integrate Middleware Functions:** Leverage the AI SDK’s middleware system to enforce consistent behavior across model calls. Two key middleware to add:  
  - **Default Settings Middleware:** Implement a middleware (or use configuration) that injects default settings for all calls – for example, a consistent temperature, or a system prompt template. This ensures uniform behavior without repeating configs. (If no built-in middleware, you can wrap the model calls via a helper function that applies these defaults globally.)  
  - **Reasoning Extraction Middleware:** Enable the SDK’s `extractReasoningMiddleware` to capture the model’s reasoning steps separately ([AI SDK Core: Language Model Middleware](https://sdk.vercel.ai/docs/ai-sdk-core/middleware#:~:text=,streaming%20language%20models)). Many advanced models (like Claude) can provide a chain-of-thought if prompted with special tags. Configure `extractReasoningMiddleware({ tagName: 'think' })` and wrap the Claude model with it ([AI SDK Core: Language Model Middleware](https://sdk.vercel.ai/docs/ai-sdk-core/middleware#:~:text=import%20,from%20%27ai)). This will automatically parse out `<think>...<think>` segments from Claude’s output into a `reasoning` field without showing them to the end-user ([AI SDK Core: Language Model Middleware](https://sdk.vercel.ai/docs/ai-sdk-core/middleware#:~:text=,streaming%20language%20models)). The extracted reasoning can later be displayed in the UI or logged for debugging.  
  - (Optional) If using models that don’t natively separate reasoning, consider `startWithReasoning: true` in the middleware config to force a reasoning tag at the beginning ([AI SDK Core: Language Model Middleware](https://sdk.vercel.ai/docs/ai-sdk-core/middleware#:~:text=The%20,see%20the%20DeepSeek%20R1%20guide)).  
  *Dependency:* The middleware should be set up when initializing the model/provider (using `wrapLanguageModel` if required). Ensure this is done during provider setup so all subsequent calls include the middleware.

- [ ] **Ensure Secure Key Management:** Verify that API keys and other secrets are handled securely. Use the `.env` file or platform secrets store to supply keys (as done for `ANTHROPIC_API_KEY` in the example env) ([bolt.new/env.example at main · open-bolt/bolt.new · GitHub](https://github.com/open-bolt/bolt.new/blob/main/env.example#:~:text=API%20Keys)). Double-check that keys are **not** exposed on the client side (if running in-browser, use a proxy or serverless function for AI calls). If the project uses Cloudflare Workers (Wrangler) or Vercel Edge functions, configure the secrets in those environments accordingly.  
  *Subtasks:* 
  - [ ] Load keys from env in the provider setup (e.g., Anthropic SDK will read `ANTHROPIC_API_KEY`). 
  - [ ] Document any new env vars (e.g., if adding `OPENAI_API_KEY` for future use) in the README or .env.example for developers.  
  - [ ] Test with a valid Claude API key to ensure the integration works end-to-end. Avoid committing any real keys to the repository.

- [ ] **Implement Fallback Mechanisms:** Design the system to be robust if the default AI model or a tool fails. For instance, configure a fallback AI provider or model in case Claude is unavailable or returns an error. The Vercel SDK supports specifying a `fallbackProvider` in custom provider setups ([AI SDK Core: Provider Management](https://sdk.vercel.ai/docs/ai-sdk-core/provider-management#:~:text=)) – consider using this to fall back to OpenAI if an Anthropics call fails unexpectedly. If not using an automatic fallback, at least handle errors from the AI generation calls (e.g., network issues or provider errors) by retrying or showing a message, rather than crashing. This aligns with the PRD’s risk mitigation to use the SDK’s unified interface to abstract differences and implement fallbacks ([GitHub - open-bolt/bolt.new: Prompt, run, edit, and deploy full-stack web applications](https://github.com/open-bolt/bolt.new#:~:text=,differences%20and%20implement%20fallback%20mechanisms)).  
  *Dependency:* Availability of an alternate provider (OpenAI key configured) for full automatic fallback. If not available, ensure graceful error messaging is in place.

- [ ] **Modularize Code Structure:** Refactor or organize new code into logical modules for maintainability. For example, create separate files or directories for `providers` (handling the registry and model init), `tools` (tool definitions and exports), and `middleware` (any custom middleware or wrappers). Ensure that the UI components remain separate from the core logic (clean separation of concerns). This modular approach will make future updates (adding providers or tools) much easier.  
  *Subtasks:* 
  - [ ] Abstract provider configuration into a module that can be extended (e.g., adding a new provider requires editing one registry config file).  
  - [ ] Similarly, keep tool definitions in a central place. This could be an array or object of tools imported by the main agent logic.  
  - [ ] Verify that the data flow is secure and clear: inputs from UI → AI SDK call (with providers/tools) → outputs (parts) → UI. With this structure in place, the core logic is insulated, and changes (like adding MCP support later) won’t require a complete rewrite.

## Milestone 2: UI/UX Updates

- [ ] **Adapt Chat UI for Streaming & Multi-part Messages:** Upgrade the frontend to fully leverage the SDK’s streaming and message-part features for a richer chat experience.  
  - [ ] **Use Message `parts` for Rendering:** Replace any logic that assumed a single text blob response with one that iterates over message **parts**. In the new SDK, each AI message can contain multiple parts (e.g. text, reasoning, tool invocations, results, sources) ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=The%20UI%20messages%20have%20a,flexible%20and%20complex%20chat%20UIs)). Update the chat component to build the message UI by mapping through `message.parts` instead of `message.content` ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=The%20UI%20messages%20have%20a,flexible%20and%20complex%20chat%20UIs)). This allows more flexible and structured outputs. For example, text parts can be shown normally, tool result parts could be formatted (like code or data), and reasoning parts could be hidden or styled differently.  
    *Tip:* The SDK’s UI guide recommends using the `parts` property for rendering specifically because it supports different message types like tool outputs ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=The%20UI%20messages%20have%20a,flexible%20and%20complex%20chat%20UIs)). This change will enable showing the AI’s reasoning or actions clearly separated from final answers.  
  - [ ] **Enable Real-time Streaming Display:** Ensure the UI shows tokens or message parts as they stream in, rather than waiting for the full response. The Vercel SDK’s `useChat` hook (or the underlying streaming utilities) can push updates to the UI as the AI responds ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=In%20the%20,displayed%20in%20the%20chat%20UI)). Leverage this by, for instance, showing a partial message bubble that updates with new text content live. Confirm that the `status` from `useChat` (e.g. `streaming` state) is handled – you might show a spinner or “Typing…” indicator when streaming starts and finalize the message when streaming ends ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=In%20the%20,displayed%20in%20the%20chat%20UI)) ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=This%20enables%20a%20seamless%20chat,entire%20response%20to%20be%20received)). The goal is a smooth experience where the user sees Claude’s answer appear word-by-word or part-by-part in real time, just like ChatGPT or other live AI chats.  
    *Implementation notes:* The `useChat` hook returns a `messages` array that updates in real-time. Continue using that or a similar mechanism, and test that long answers stream properly.  
  - [ ] **Display Reasoning and Tool Outputs:** Update the UI to handle special part types such as `reasoning` and tool results. For example, if using `extractReasoningMiddleware`, the AI’s reasoning might come through as a part of type `"reasoning"` separate from the final answer. Decide on a UI approach: possibly render reasoning text in a lighter font or within a collapsible section for users who want to see “AI thoughts.” Similarly, if a tool produces a result (say an image or data), ensure the UI can render that part (e.g., showing an image thumbnail for an image part, or formatted JSON for a data part). The SDK’s UI documentation provides examples of filtering and rendering different part types (text vs. source vs. file, etc.) ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=match%20at%20L988%20On%20the,the%20bottom%20of%20the%20message)) ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=)) – follow those patterns to cover all relevant types.  
    *Subtasks:* 
    - [ ] Implement conditional rendering for part types: e.g., `if (part.type === 'reasoning') { /* render reasoning UI */ }` ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=%2F%2F%20reasoning%20parts%3A)), `if (part.type === 'tool_result') { /* render tool result UI */ }`, etc.  
    - [ ] If tool results include references or links (such as a URL or a code snippet), make those interactive (clickable link, copyable code, etc.) to enhance usability.  
    - [ ] Test with a scenario where Claude uses a tool (the multi-step flow) and ensure the UI shows a sequence like: the AI’s thought (if exposed), the tool call result, and then the final answer. The messages should be clearly segmented.

- [ ] **Error Handling & Messages in UI:** Improve the frontend to gracefully handle and display errors from the AI or tools.  
  - [ ] **Show AI/Tool Errors to the User:** When an error occurs (e.g., the AI SDK returns an `error` state or a tool throws an exception), surface a user-friendly notification. This could be a message bubble from the “system” or AI saying, e.g., “⚠️ An error occurred while generating the response.” Use the SDK’s error object available in the `error` state of `useChat` ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=Similarly%2C%20the%20,or%20show%20a%20retry%20button)) to get details. For instance, distinguish between different error types if possible: if it’s a `NoSuchToolError` you might say “(The AI tried to use an unknown tool – please retry)”, whereas a provider API error might say “(The AI service is currently unavailable.)”. By handling these cases, the UI won’t just silently fail or hang; the user will know something went wrong ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=Similarly%2C%20the%20,or%20show%20a%20retry%20button)).  
  - [ ] **Visual Feedback for Errors:** Style error messages or states distinctively. For example, render error text in red or with a warning icon, and possibly disable the input temporarily if the error is not recoverable. The SDK’s UI guide notes that the `error` state can be used to, for example, disable the submit button or show a retry option ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=Similarly%2C%20the%20,or%20show%20a%20retry%20button)). Implement a “Retry” button or allow the user to resend their prompt if appropriate when an error occurs.  
  - [ ] **Log Errors for Debugging:** (For development) ensure that errors are also logged to the console or a monitoring service, so developers can address the underlying issues. This is especially important during Beta to catch unhandled cases.

- [ ] **Polish Chat UX & Controls:** Add any additional UI enhancements to improve usability and clarity.  
  - [ ] If not already present, include a loading indicator or spinner when the AI is thinking (during `status === 'submitted'` or `streaming`) ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=Status)) ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=You%20can%20use%20,the%20following%20purposes)). This gives the user immediate feedback that their request is being processed.  
  - [ ] Provide a “Stop Generation” button while streaming (leveraging the `stop()` function from `useChat`) so users can halt long responses ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=You%20can%20use%20,the%20following%20purposes)).  
  - [ ] Ensure the input box is disabled while streaming to prevent overlapping requests, then re-enabled when the AI is ready (the SDK’s `status` flags can help manage this) ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=The%20,has%20the%20following%20possible%20values)) ([AI SDK UI: Chatbot](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot#:~:text=import%20,sdk%2Freact)).  
  - [ ] Review the overall styling of the multi-part messages — for example, differentiate user vs AI messages (e.g., different background color), and distinguish any special content like code or images within the AI message. The new multi-part structure may include richer content, so adjust CSS accordingly (e.g., ensure long tool outputs scroll or wrap properly).  
  - [ ] Test the UI thoroughly with various scenarios: a normal Q&A with just text, a prompt that triggers tool use and reasoning, an error scenario (like disabling network or providing a bad tool call). Confirm that in each case the interface remains clear and responsive.

## Milestone 3: Beta Release & Feedback

- [ ] **Deploy Beta Version:** Package the updated application and deploy it to a testing environment (e.g., Vercel, Netlify, or a preview URL on the StackBlitz WebContainer if accessible to testers). Ensure that all environment variables (Anthropic API key, etc.) are correctly set in the deployment target. The goal is to have a working beta that closely resembles the intended final product for user evaluation.

- [ ] **Invite a Test User Group:** Identify a small group of users (or internal team members) to use the Bolt.new beta. Provide them with instructions on new features (e.g., “You can now use tools like X,” “The AI may show its reasoning steps,” etc.) so they know what to expect. Encourage them to experiment with writing code via the AI, triggering tools, and observing the new UI elements.

- [ ] **Collect Feedback & Bug Reports:** Set up channels for beta users to report issues or suggest improvements (this could be a shared document, GitHub issues, or a feedback form). Pay special attention to:
  - Any errors or crashes that occur during usage (these should also be in logs). 
  - Confusing UI elements (e.g., “the reasoning text is distracting” or “error message not clear”).
  - Feature requests (e.g., support for another provider or tool) that align with future goals.
  - Performance feedback (if responses feel slow or UI lags, noting it for optimization).
  
- [ ] **Monitor Performance & Logs:** During the beta, use logging/analytics to monitor how the system performs. Check if multi-step interactions are completing within reasonable time, and if any tool calls are failing frequently. This will help identify any bottlenecks or necessary fallbacks (per the PRD’s performance risk notes, keep an eye on latency with streaming and multi-step) ([GitHub - open-bolt/bolt.new: Prompt, run, edit, and deploy full-stack web applications](https://github.com/open-bolt/bolt.new#:~:text=,step%20interactions%20and%20streaming%20responses)).

- [ ] **Triage and Address Critical Issues:** For any high-priority bugs uncovered in beta, fix them in a timely manner. For example, if a certain prompt consistently breaks the agent or a tool, patch that (or temporarily disable that tool) before final release. If UI feedback suggests minor tweaks (e.g., hiding the reasoning by default), implement those changes now. Less critical feedback can be noted for the post-release roadmap.

- [ ] **Update Documentation (Incremental):** Start drafting any documentation changes based on beta feedback. For instance, if testers found it hard to discover a feature, plan to add a note in the README or an in-app hint. Although final docs will be completed in the next phase, having a running list now is helpful.

## Milestone 4: Final Release

- [ ] **Resolve Remaining Bugs:** Go through all outstanding issues from the beta and any final QA pass. Ensure that all functional requirements from the PRD are met – e.g., provider switching works (if two providers configured), tool calling with multi-steps works reliably, UI is stable during streaming, and errors are handled. Pay special attention to edge cases discovered late (e.g., rapid successive prompts, extremely long responses, etc.) and handle them gracefully (possibly with final adjustments to `maxTokens` or timeouts if needed).

- [ ] **Code Cleanup & Review:** Refactor any rough edges in the code for clarity and maintainability. Remove extraneous logs or test code that was added during development. Double-check that the code is modular and comments/docs are added wherever the implementation might be non-obvious (like the middleware setup or any tricky workaround). This makes the open-source repo easier to understand for new contributors. Also, ensure that secure info (keys, etc.) are not exposed anywhere – they should all be loaded from env at runtime.

- [ ] **Finalize Documentation:** Update the repository README (and any other docs) to reflect the new upgrade. This includes:
  - Documenting how to configure providers (e.g., “By default, the AI uses Claude. To use OpenAI, set an OPENAI_API_KEY and switch the provider ID in config.”).
  - Describing available tools and their usage, if any are built-in (“The agent can call a weather tool automatically when needed,” etc.).
  - Noting the new UI features (streaming responses, reasoning display toggle if added, etc.).
  - Updating any quick-start or development instructions to cover the Vercel AI SDK setup, required environment variables, and running tests.
  - Include links to the Vercel AI SDK docs for advanced customization, so developers can extend the project beyond the default (e.g., adding more tools or providers in the future).

- [ ] **Security & Key Handling Audit:** Before release, audit the project for any security concerns. Ensure that API keys are referenced safely and are required from env (the app should clearly error out if no key is provided, guiding the user to add one). Implement any last-minute fallback or limits – for example, if no Anthropics key is present, perhaps the app can default to a demo mode or prompt the user to input one, rather than just breaking. This provides a robust experience for open-source users who clone the repo.

- [ ] **Performance Optimization:** If any performance issues were noted, do a final optimization pass. This could mean adjusting the streaming throttle, optimizing the rendering of message parts (e.g., avoid re-rendering entire list on each new token by keying properly), or tweaking `maxSteps`/prompt formats to reduce latency. The app should feel snappy and reliable for the end user.

- [ ] **Release Deployment:** Deploy the final version of Bolt.new with all changes. This may involve pushing to production environment and verifying that the deployment has all necessary config (correct env vars, etc.). Perform a final smoke-test on the live environment to ensure everything (especially the integration with Claude via the Vercel SDK) works as in development.

- [ ] **Open Source Release (if applicable):** Since the PRD mentions switching from private to public, ensure the GitHub repository is made public once everything is ready ([GitHub - open-bolt/bolt.new: Prompt, run, edit, and deploy full-stack web applications](https://github.com/open-bolt/bolt.new#:~:text=,Final%20Release%20%281%20week)). Before this, scrub any sensitive data from commit history if needed (keys, tokens used during testing, etc.). Create a concise changelog or release notes summarizing the upgrade (for those following the project). Tag a version (e.g., v1.0.0 for the new release) in GitHub.

- [ ] **Post-release Monitoring and Fallbacks:** After final release, keep an eye on the application’s stability. Thanks to the unified provider layer, if Anthropics Claude faces an outage or rate limit, you can quickly switch the default model (or instruct users how to do so) to another provider with minimal changes ([GitHub - open-bolt/bolt.new: Prompt, run, edit, and deploy full-stack web applications](https://github.com/open-bolt/bolt.new#:~:text=,differences%20and%20implement%20fallback%20mechanisms)). The architecture put in place (registry, middleware, etc.) should make such changes straightforward. Document any such operational tips for the maintainers.
